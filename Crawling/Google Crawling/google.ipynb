{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import requests\n",
    "import random\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Crawl HTML of google search results\n",
    "def get_google_list_html(num, key):\n",
    "    r = requests.get('https://www.google.co.in/search?hl=ko&output=search&q='+unicode(key,'cp949')+'&tbm=nws&tbs=qdr:d&start='+str(num))\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To get what we want from the HTML... News Title, Date, Reference, and links for each article.\n",
    "def get_google_article(html):    \n",
    "    \n",
    "    article_list = []\n",
    "    \n",
    "    tot_article = html.find_all(class_='g')\n",
    "    for i in range(0,len(tot_article)):\n",
    "        flag = 1\n",
    "        try:\n",
    "            tmp_title = html.find_all(class_='g')[i]\n",
    "            tmp_test = tmp_title.find_all('a')\n",
    "            if (len(tmp_test) < 2):\n",
    "                flag = 0\n",
    "            else:\n",
    "                flag = 1\n",
    "\n",
    "            tmp_link = tmp_test[0]\n",
    "            if flag == 1:\n",
    "                for j in range(0, 10):\n",
    "                    test_str = tmp_title.find_all('a')[j].get_text()\n",
    "                    article_title = test_str.encode('utf-8')\n",
    "                    tmp_link = tmp_test[j]\n",
    "                    article_link = tmp_link.encode('utf-8').split('\\\"')[1].split('&amp')[0].split('/url?q=')[1].replace('%3F','?')\n",
    "                    article_link = article_link.replace('%3D','=')\n",
    "                    article_link = article_link.replace('%26','&')\n",
    "                    article_ref = unicode(tmp_title.find_all(class_='slp')[j].get_text().split(' - ')[0])\n",
    "                    if len(article_title) == 0:\n",
    "                        break\n",
    "                    \n",
    "                    article = {'구분': '뉴스', '발표처' : article_ref, \n",
    "                               '웹주소' : '=HYPERLINK(\\\"'+article_link+\"\\\")\", \n",
    "                               '제목' : article_title}\n",
    "                    article_list.append(article)\n",
    "\n",
    "            else: \n",
    "                test_str = tmp_title.find_all('a')[0].get_text()\n",
    "                #test = unicode(test_str, 'utf-8')\n",
    "                article_title = test_str.encode('utf-8')\n",
    "                article_link = tmp_link.encode('utf-8').split('\\\"')[1].split('&amp')[0].split('/url?q=')[1].replace('%3F','?')\n",
    "                article_link = article_link.replace('%3D','=')\n",
    "                article_link = article_link.replace('%26','&')\n",
    "                article_ref = unicode(tmp_title.find_all(class_='slp')[0].get_text().split(' - ')[0])\n",
    "\n",
    "                article = {'구분': '뉴스', '발표처' : article_ref, \n",
    "                            '웹주소' : '=HYPERLINK(\\\"'+article_link+\"\\\")\",\n",
    "                           '제목' : article_title}\n",
    "                article_list.append(article)\n",
    "                #print test_str\n",
    "            \"\"\"\n",
    "            test_str = tmp_title.find_all('a')[0].get_text()\n",
    "            #test = unicode(test_str, 'utf-8')\n",
    "            article_title = test_str.encode('utf-8')\n",
    "            article_link = tmp_link.encode('utf-8').split('\\\"')[1].split('&amp')[0].split('/url?q=')[1].replace('%3F','?')\n",
    "            article_link = article_link.replace('%3D','=')\n",
    "            article_ref = unicode(tmp_title.find_all(class_='slp')[0].get_text().split(' - ')[0])\n",
    "\n",
    "            article = {'구분': '뉴스', '발표처' : article_ref, '웹주소' : article_link, '제목' : article_title}\n",
    "            article_list.append(article)\n",
    "            #print test_str\n",
    "            \"\"\"\n",
    "        except:\n",
    "            continue\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WriteDictToCSV(csv_file,csv_columns,dict_data):\n",
    "    try:\n",
    "        with open(csv_file, 'wb') as csvfile:\n",
    "            csvfile.write(u'\\ufeff'.encode('utf-8').strip())\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "            writer.writeheader()\n",
    "            for data in dict_data:\n",
    "                writer.writerow({k:v.encode('utf-8').strip() for k,v in data.items()})\n",
    "    except IOError as (errno, strerror):\n",
    "            print(\"I/O error({0}): {1}\".format(errno, strerror)) \n",
    "    return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_list = []\n",
    "tmp_list = []\n",
    "search_key = []\n",
    "\n",
    "f = open(\"keyword.txt\", 'r')\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if not line: \n",
    "        break\n",
    "    search_key.append(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(0, len(search_key)):\n",
    "    for i in range(0,20):\n",
    "        delay = random.randrange(10,30)\n",
    "        html = get_google_list_html(i*10, search_key[j])\n",
    "        tmp_list = get_google_article(html)\n",
    "        if len(tmp_list) == 0:\n",
    "            break\n",
    "        google_list = google_list + tmp_list\n",
    "        time.sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = str(datetime.today()).split(\" \")[0]\n",
    "csv_columns = ['구분', '발표처', '제목', '웹주소']\n",
    "currentPath = os.getcwd()\n",
    "csv_file = \"google-\"+str(date)+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WriteDictToCSV(csv_file,csv_columns,google_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
